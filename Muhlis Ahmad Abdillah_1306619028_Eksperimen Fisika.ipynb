{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kaggle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading heartbeat-sounds.zip to c:\\Users\\Muhlis\\OneDrive - Universitas Negeri Jakarta (UNJ)\\Documents\\Eksperimen Fisika\\Heartbeat-Sound-Classification\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/110M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/110M [00:00<01:07, 1.68MB/s]\n",
      "  2%|▏         | 2.00M/110M [00:01<01:03, 1.78MB/s]\n",
      "  3%|▎         | 3.00M/110M [00:01<01:03, 1.76MB/s]\n",
      "  4%|▎         | 4.00M/110M [00:02<01:03, 1.76MB/s]\n",
      "  5%|▍         | 5.00M/110M [00:02<01:01, 1.80MB/s]\n",
      "  5%|▌         | 6.00M/110M [00:03<00:59, 1.83MB/s]\n",
      "  6%|▋         | 7.00M/110M [00:04<01:05, 1.66MB/s]\n",
      "  7%|▋         | 8.00M/110M [00:04<01:05, 1.65MB/s]\n",
      "  8%|▊         | 9.00M/110M [00:05<01:05, 1.61MB/s]\n",
      "  9%|▉         | 10.0M/110M [00:06<01:04, 1.63MB/s]\n",
      " 10%|▉         | 11.0M/110M [00:06<01:02, 1.66MB/s]\n",
      " 11%|█         | 12.0M/110M [00:07<01:02, 1.63MB/s]\n",
      " 12%|█▏        | 13.0M/110M [00:08<01:00, 1.67MB/s]\n",
      " 13%|█▎        | 14.0M/110M [00:08<01:01, 1.65MB/s]\n",
      " 14%|█▎        | 15.0M/110M [00:09<00:58, 1.70MB/s]\n",
      " 15%|█▍        | 16.0M/110M [00:09<00:55, 1.77MB/s]\n",
      " 15%|█▌        | 17.0M/110M [00:10<00:52, 1.85MB/s]\n",
      " 16%|█▋        | 18.0M/110M [00:10<00:52, 1.85MB/s]\n",
      " 17%|█▋        | 19.0M/110M [00:11<00:54, 1.75MB/s]\n",
      " 18%|█▊        | 20.0M/110M [00:12<00:51, 1.82MB/s]\n",
      " 19%|█▉        | 21.0M/110M [00:12<00:51, 1.82MB/s]\n",
      " 20%|█▉        | 22.0M/110M [00:13<00:49, 1.85MB/s]\n",
      " 21%|██        | 23.0M/110M [00:13<00:48, 1.86MB/s]\n",
      " 22%|██▏       | 24.0M/110M [00:14<00:47, 1.92MB/s]\n",
      " 23%|██▎       | 25.0M/110M [00:14<00:44, 2.02MB/s]\n",
      " 24%|██▎       | 26.0M/110M [00:15<00:43, 2.05MB/s]\n",
      " 25%|██▍       | 27.0M/110M [00:15<00:42, 2.07MB/s]\n",
      " 25%|██▌       | 28.0M/110M [00:16<00:41, 2.07MB/s]\n",
      " 26%|██▋       | 29.0M/110M [00:16<00:40, 2.09MB/s]\n",
      " 27%|██▋       | 30.0M/110M [00:17<00:40, 2.08MB/s]\n",
      " 28%|██▊       | 31.0M/110M [00:17<00:39, 2.09MB/s]\n",
      " 29%|██▉       | 32.0M/110M [00:18<00:38, 2.11MB/s]\n",
      " 30%|██▉       | 33.0M/110M [00:18<00:38, 2.09MB/s]\n",
      " 31%|███       | 34.0M/110M [00:19<00:38, 2.08MB/s]\n",
      " 32%|███▏      | 35.0M/110M [00:19<00:38, 2.04MB/s]\n",
      " 33%|███▎      | 36.0M/110M [00:20<00:39, 1.98MB/s]\n",
      " 34%|███▎      | 37.0M/110M [00:20<00:38, 1.98MB/s]\n",
      " 35%|███▍      | 38.0M/110M [00:21<00:38, 1.98MB/s]\n",
      " 35%|███▌      | 39.0M/110M [00:22<00:41, 1.81MB/s]\n",
      " 36%|███▋      | 40.0M/110M [00:22<00:40, 1.83MB/s]\n",
      " 37%|███▋      | 41.0M/110M [00:23<00:39, 1.83MB/s]\n",
      " 38%|███▊      | 42.0M/110M [00:23<00:37, 1.90MB/s]\n",
      " 39%|███▉      | 43.0M/110M [00:24<00:36, 1.90MB/s]\n",
      " 40%|███▉      | 44.0M/110M [00:24<00:35, 1.93MB/s]\n",
      " 41%|████      | 45.0M/110M [00:25<00:34, 1.98MB/s]\n",
      " 42%|████▏     | 46.0M/110M [00:25<00:33, 1.98MB/s]\n",
      " 43%|████▎     | 47.0M/110M [00:26<00:33, 2.01MB/s]\n",
      " 44%|████▎     | 48.0M/110M [00:26<00:32, 2.01MB/s]\n",
      " 44%|████▍     | 49.0M/110M [00:27<00:32, 1.98MB/s]\n",
      " 45%|████▌     | 50.0M/110M [00:28<00:34, 1.84MB/s]\n",
      " 46%|████▋     | 51.0M/110M [00:28<00:36, 1.71MB/s]\n",
      " 47%|████▋     | 52.0M/110M [00:29<00:38, 1.59MB/s]\n",
      " 48%|████▊     | 53.0M/110M [00:30<00:36, 1.65MB/s]\n",
      " 49%|████▉     | 54.0M/110M [00:30<00:35, 1.67MB/s]\n",
      " 50%|████▉     | 55.0M/110M [00:31<00:32, 1.77MB/s]\n",
      " 51%|█████     | 56.0M/110M [00:31<00:32, 1.76MB/s]\n",
      " 52%|█████▏    | 57.0M/110M [00:32<00:30, 1.83MB/s]\n",
      " 53%|█████▎    | 58.0M/110M [00:32<00:30, 1.81MB/s]\n",
      " 54%|█████▎    | 59.0M/110M [00:33<00:29, 1.82MB/s]\n",
      " 54%|█████▍    | 60.0M/110M [00:34<00:29, 1.80MB/s]\n",
      " 55%|█████▌    | 61.0M/110M [00:34<00:27, 1.88MB/s]\n",
      " 56%|█████▋    | 62.0M/110M [00:35<00:26, 1.92MB/s]\n",
      " 57%|█████▋    | 63.0M/110M [00:35<00:25, 1.92MB/s]\n",
      " 58%|█████▊    | 64.0M/110M [00:36<00:27, 1.76MB/s]\n",
      " 59%|█████▉    | 65.0M/110M [00:37<00:27, 1.71MB/s]\n",
      " 60%|█████▉    | 66.0M/110M [00:37<00:25, 1.80MB/s]\n",
      " 61%|██████    | 67.0M/110M [00:38<00:24, 1.87MB/s]\n",
      " 62%|██████▏   | 68.0M/110M [00:38<00:23, 1.89MB/s]\n",
      " 63%|██████▎   | 69.0M/110M [00:39<00:21, 2.02MB/s]\n",
      " 64%|██████▎   | 70.0M/110M [00:39<00:19, 2.13MB/s]\n",
      " 64%|██████▍   | 71.0M/110M [00:39<00:18, 2.21MB/s]\n",
      " 65%|██████▌   | 72.0M/110M [00:40<00:17, 2.25MB/s]\n",
      " 66%|██████▋   | 73.0M/110M [00:40<00:18, 2.15MB/s]\n",
      " 67%|██████▋   | 74.0M/110M [00:41<00:17, 2.17MB/s]\n",
      " 68%|██████▊   | 75.0M/110M [00:41<00:18, 2.02MB/s]\n",
      " 69%|██████▉   | 76.0M/110M [00:42<00:18, 1.89MB/s]\n",
      " 70%|██████▉   | 77.0M/110M [00:43<00:19, 1.82MB/s]\n",
      " 71%|███████   | 78.0M/110M [00:43<00:19, 1.74MB/s]\n",
      " 72%|███████▏  | 79.0M/110M [00:44<00:18, 1.72MB/s]\n",
      " 73%|███████▎  | 80.0M/110M [00:45<00:17, 1.76MB/s]\n",
      " 74%|███████▎  | 81.0M/110M [00:45<00:17, 1.74MB/s]\n",
      " 74%|███████▍  | 82.0M/110M [00:46<00:16, 1.80MB/s]\n",
      " 75%|███████▌  | 83.0M/110M [00:46<00:15, 1.80MB/s]\n",
      " 76%|███████▋  | 84.0M/110M [00:47<00:16, 1.62MB/s]\n",
      " 77%|███████▋  | 85.0M/110M [00:48<00:15, 1.66MB/s]\n",
      " 78%|███████▊  | 86.0M/110M [00:48<00:14, 1.75MB/s]\n",
      " 79%|███████▉  | 87.0M/110M [00:49<00:13, 1.86MB/s]\n",
      " 80%|███████▉  | 88.0M/110M [00:49<00:12, 1.91MB/s]\n",
      " 81%|████████  | 89.0M/110M [00:50<00:11, 1.94MB/s]\n",
      " 82%|████████▏ | 90.0M/110M [00:50<00:10, 1.94MB/s]\n",
      " 83%|████████▎ | 91.0M/110M [00:51<00:10, 1.98MB/s]\n",
      " 84%|████████▎ | 92.0M/110M [00:51<00:09, 1.98MB/s]\n",
      " 84%|████████▍ | 93.0M/110M [00:52<00:09, 1.82MB/s]\n",
      " 85%|████████▌ | 94.0M/110M [00:53<00:09, 1.84MB/s]\n",
      " 86%|████████▋ | 95.0M/110M [00:53<00:08, 1.86MB/s]\n",
      " 87%|████████▋ | 96.0M/110M [00:54<00:07, 1.92MB/s]\n",
      " 88%|████████▊ | 97.0M/110M [00:54<00:07, 1.96MB/s]\n",
      " 89%|████████▉ | 98.0M/110M [00:55<00:06, 2.03MB/s]\n",
      " 90%|████████▉ | 99.0M/110M [00:55<00:05, 2.02MB/s]\n",
      " 91%|█████████ | 100M/110M [00:56<00:05, 1.98MB/s] \n",
      " 92%|█████████▏| 101M/110M [00:56<00:04, 1.99MB/s]\n",
      " 93%|█████████▎| 102M/110M [00:57<00:04, 1.95MB/s]\n",
      " 94%|█████████▎| 103M/110M [00:57<00:03, 1.98MB/s]\n",
      " 94%|█████████▍| 104M/110M [00:58<00:03, 1.84MB/s]\n",
      " 95%|█████████▌| 105M/110M [00:58<00:02, 1.89MB/s]\n",
      " 96%|█████████▋| 106M/110M [00:59<00:02, 2.00MB/s]\n",
      " 97%|█████████▋| 107M/110M [00:59<00:01, 2.02MB/s]\n",
      " 98%|█████████▊| 108M/110M [01:00<00:01, 2.00MB/s]\n",
      " 99%|█████████▉| 109M/110M [01:00<00:00, 2.02MB/s]\n",
      "100%|█████████▉| 110M/110M [01:01<00:00, 2.03MB/s]\n",
      "100%|██████████| 110M/110M [01:01<00:00, 1.88MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d \"kinguistics/heartbeat-sounds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = \"heartbeat-sounds.zip\"\n",
    "with ZipFile(file_name, 'r') as zip_file:\n",
    "  zip_file.printdir()\n",
    "  print('Extracting all the files now...')\n",
    "  zip_file.extractall()\n",
    "  print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_a = pd.read_csv(\"./set_a.csv\")\n",
    "set_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_b_directory_list = os.listdir(\"./set_b\")\n",
    "\n",
    "file_label = []\n",
    "file_path = []\n",
    "\n",
    "for file in set_b_directory_list:\n",
    "    part = file.split('_')\n",
    "    if part[0] == 'extrastole':\n",
    "        file_path.append('set_b/'+file)\n",
    "        file_label.append('extrastole')\n",
    "    elif part[0] == 'murmur':\n",
    "        file_path.append('set_b/'+file)\n",
    "        file_label.append('murmur')\n",
    "    elif part[0] == 'normal':\n",
    "        file_path.append('set_b/'+file)\n",
    "        file_label.append('normal')\n",
    "label_df = pd.DataFrame(file_label, columns=['label'])\n",
    "path_df = pd.DataFrame(file_path, columns=['fname'])\n",
    "set_b = pd.concat([path_df, label_df], axis=1)\n",
    "set_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_a.drop([\"sublabel\",\"dataset\"],axis=\"columns\",inplace=True)\n",
    "data_ab = pd.concat([set_a, set_b])\n",
    "data_ab = data_ab.dropna()\n",
    "data_ab = data_ab.reset_index()\n",
    "data_ab.drop([\"index\"],axis=\"columns\",inplace=True)\n",
    "data_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ab.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ab.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waveplot(data, samplingrate, label:str):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.title(f\"Waveplot audio untuk kondisi jantung : {label}\", fontsize=25, pad=20)\n",
    "    librosa.display.waveshow(data, sr = samplingrate)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pembagian dataset:\")\n",
    "print(f\"  - data a = {len(data_ab[data_ab['fname'].str.contains('set_a/')])}\")\n",
    "print(f\"  - data b = {len(data_ab[data_ab['fname'].str.contains('set_b/')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jumlah label kondisi jantung =\",str(len(data_ab.label.unique())),\"kategori, diantaranya:\")\n",
    "\n",
    "for i in data_ab.label.unique():\n",
    "    print(f\"- {i}\\t: {data_ab['label'].value_counts()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_ab.fname[data_ab.label==data_ab.label.unique()[0]][19]\n",
    "data = librosa.load(path)[0]\n",
    "rate = wavfile.read(path)[0]\n",
    "create_waveplot(data, rate, data_ab.label.unique()[0])\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_ab.fname[data_ab.label==data_ab.label.unique()[1]][50]\n",
    "data = librosa.load(path)[0]\n",
    "rate = wavfile.read(path)[0]\n",
    "create_waveplot(data, rate, data_ab.label.unique()[1])\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_ab.fname[data_ab.label==data_ab.label.unique()[2]][63]\n",
    "data = librosa.load(path)[0]\n",
    "rate = wavfile.read(path)[0]\n",
    "create_waveplot(data, rate, data_ab.label.unique()[2])\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_ab.fname[data_ab.label==data_ab.label.unique()[3]][97]\n",
    "data = librosa.load(path)[0]\n",
    "rate = wavfile.read(path)[0]\n",
    "create_waveplot(data, rate, data_ab.label.unique()[3])\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_ab.fname[data_ab.label==data_ab.label.unique()[4]][146]\n",
    "data = librosa.load(path)[0]\n",
    "rate = wavfile.read(path)[0]\n",
    "create_waveplot(data, rate, data_ab.label.unique()[4])\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_value = 0.009*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_value*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.6):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "def shift(data, samplingrate):\n",
    "    return np.roll(data,samplingrate/10)\n",
    "\n",
    "def pitch(data, samplingrate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, samplingrate, pitch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, samplerate):\n",
    "    # ZCR\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=samplerate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=samplerate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=samplerate).T, axis=0)\n",
    "    result = np.hstack((result, mel)) # stacking horizontally\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(path):\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data = librosa.load(path)[0]\n",
    "    rate = wavfile.read(path)[0]\n",
    "    \n",
    "    # without augmentation\n",
    "    res1 = extract_features(data, rate)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    # data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_features(noise_data, rate)\n",
    "    result = np.vstack((result, res2)) # stacking vertically\n",
    "    \n",
    "    # data with stretching and pitching\n",
    "    new_data = stretch(data)\n",
    "    data_stretch_pitch = pitch(new_data, rate)\n",
    "    res3 = extract_features(data_stretch_pitch, rate)\n",
    "    result = np.vstack((result, res3)) # stacking vertically\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for path, label in zip(data_ab.fname, data_ab.label):\n",
    "    feature = get_features(path)\n",
    "    for element in feature:\n",
    "        X.append(element)\n",
    "        Y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = pd.DataFrame(X)\n",
    "Features['label'] = Y\n",
    "Features.to_csv('features.csv', index=False)\n",
    "Features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jumlah data fitur setiap label =\",str(len(data_ab.label.unique())),\"kategori, diantaranya:\")\n",
    "\n",
    "for i in Features.label.unique():\n",
    "    print(f\"- {i}\\t: {Features['label'].value_counts()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Features.iloc[:,:-1].values\n",
    "Y = Features['label'].values\n",
    "enc = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "Y_Encode = enc.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ukuran data fitur : {X.shape}\")\n",
    "print(f\"Ukuran data label : {Y_Encode.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y_Encode, train_size=0.8, random_state=4, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train,axis=2)\n",
    "x_test = np.expand_dims(x_test,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ukuran data train : {x_train.shape}\")\n",
    "print(f\"Ukuran data test : {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Early_Stopper = EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, verbose=0, patience=2, min_lr=0.001)\n",
    "Checkpoint_Model = ModelCheckpoint(monitor=\"val_accuracy\",\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   filepath=\"./modelcheck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=4, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=4, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=4, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=4, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv1D_Model = model.fit(x_train, y_train, batch_size=12, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp, Checkpoint_Model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LOSS\\t\\t: {round(Model_Results[0]*100,2)}%\")\n",
    "print(f\"ACCURACY\\t: {round(Model_Results[1]*100,2)}%\")\n",
    "\n",
    "epochs = [i for i in range(50)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = Conv1D_Model.history['accuracy']\n",
    "train_loss = Conv1D_Model.history['loss']\n",
    "test_acc = Conv1D_Model.history['val_accuracy']\n",
    "test_loss = Conv1D_Model.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_test)\n",
    "y_pred = enc.inverse_transform(pred_test)\n",
    "y_test = enc.inverse_transform(y_test)\n",
    "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df['Predicted Labels'] = y_pred.flatten()\n",
    "df['Actual Labels'] = y_test.flatten()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in enc.categories_] , columns = [i for i in enc.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=10)\n",
    "plt.ylabel('Actual Labels', size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Conv1D_model(rlrp,Checkpoint_Model)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c65eb881834d6974a88730dd1fd433cd315b84be1472a9a3aa8391dd3b826fa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
